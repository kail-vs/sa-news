{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab80e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"../..\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121df55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapipeline.utils.spark_session import get_spark_session\n",
    "\n",
    "spark = get_spark_session(\"ML_Event_Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ccb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_path = os.path.join(\n",
    "    project_root,\n",
    "    \"sanewsstorage/ml/clusters_labeled\"\n",
    ")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(clusters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8a3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "df = df.select(\n",
    "    \"bronze_hash\",\n",
    "    \"cluster_id\",\n",
    "    \"cluster_label\",\n",
    "    \"published_at\"\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"date\",\n",
    "    to_date(col(\"published_at\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73cc6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "daily_counts = (\n",
    "    df.groupBy(\"cluster_id\", \"cluster_label\", \"date\")\n",
    "    .agg(\n",
    "        count(\"bronze_hash\").alias(\"article_count\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f4d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, stddev\n",
    "\n",
    "window_spec = (\n",
    "    Window\n",
    "    .partitionBy(\"cluster_id\")\n",
    "    .orderBy(\"date\")\n",
    "    .rowsBetween(-7, -1)   # previous 7 days\n",
    ")\n",
    "\n",
    "baseline_df = daily_counts.withColumn(\n",
    "    \"baseline_avg\",\n",
    "    avg(\"article_count\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"baseline_std\",\n",
    "    stddev(\"article_count\").over(window_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "695f504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "event_df = baseline_df.withColumn(\n",
    "    \"z_score\",\n",
    "    when(\n",
    "        col(\"baseline_std\").isNull(),\n",
    "        0\n",
    "    ).otherwise(\n",
    "        (col(\"article_count\") - col(\"baseline_avg\")) /\n",
    "        col(\"baseline_std\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735598fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = event_df.withColumn(\n",
    "    \"is_event\",\n",
    "    when(col(\"z_score\") >= 2.0, 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914c154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = event_df.withColumn(\n",
    "    \"event_intensity\",\n",
    "    when(col(\"z_score\") >= 4, \"Viral\")\n",
    "    .when(col(\"z_score\") >= 2, \"Trending\")\n",
    "    .otherwise(\"Normal\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df931306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, date_sub\n",
    "\n",
    "recent_events = event_df.filter(\n",
    "    (col(\"is_event\") == 1) &\n",
    "    (col(\"date\") >= date_sub(current_date(), 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ad5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_path = os.path.join(\n",
    "    project_root,\n",
    "    \"sanewsstorage/ml/events\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0201b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, event_path):\n",
    "\n",
    "    delta_table = DeltaTable.forPath(\n",
    "        spark,\n",
    "        event_path\n",
    "    )\n",
    "\n",
    "    (\n",
    "        delta_table.alias(\"t\")\n",
    "        .merge(\n",
    "            event_df.alias(\"s\"),\n",
    "            \"t.cluster_id = s.cluster_id AND t.date = s.date\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    (\n",
    "        event_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(event_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078db101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+-------------+------------------+------------------+-------------------+--------+---------------+\n",
      "|cluster_id|       cluster_label|      date|article_count|      baseline_avg|      baseline_std|            z_score|is_event|event_intensity|\n",
      "+----------+--------------------+----------+-------------+------------------+------------------+-------------------+--------+---------------+\n",
      "|         0|2026, available, ...|2026-02-02|           58|              NULL|              NULL|                0.0|       0|         Normal|\n",
      "|         0|2026, available, ...|2026-02-03|          292|              58.0|              NULL|                0.0|       0|         Normal|\n",
      "|         0|2026, available, ...|2026-02-04|          386|             175.0|165.46298679765212| 1.2752096652167653|       0|         Normal|\n",
      "|         0|2026, available, ...|2026-02-05|          212|245.33333333333334|168.90628565371193|-0.1973480927860354|       0|         Normal|\n",
      "|         0|2026, available, ...|2026-02-06|           93|             237.0|138.91484201961046|-1.0366062971131025|       0|         Normal|\n",
      "+----------+--------------------+----------+-------------+------------------+------------------+-------------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "event_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
