{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9100897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"../..\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e97d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatapipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_session\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_spark_session\n\u001b[1;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mget_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSilver_CombineDeltas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.databricks.delta.schema.autoMerge.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.parquet.enableVectorizedReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Echelon\\Desktop\\re\\sa-news\\datapipeline\\utils\\spark_session.py:6\u001b[0m, in \u001b[0;36mget_spark_session\u001b[1;34m(app_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_spark_session\u001b[39m(app_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSA-News\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     python_exec \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[0;32m      7\u001b[0m     builder \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      8\u001b[0m         SparkSession\u001b[38;5;241m.\u001b[39mbuilder\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mappName(app_name)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     spark \u001b[38;5;241m=\u001b[39m configure_spark_with_delta_pip(builder)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "from datapipeline.utils.spark_session import get_spark_session\n",
    "\n",
    "spark = get_spark_session(\"Silver_CombineDeltas\")\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b321310",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi_path      = \"../../sanewsstorage/bronze/deltatables/newsapi_delta\"\n",
    "newsdata_path     = \"../../sanewsstorage/bronze/deltatables/newsdata_delta\"\n",
    "newsdatahub_path  = \"../../sanewsstorage/bronze/deltatables/newsdatahub_delta\"\n",
    "\n",
    "newsapi_df     = spark.read.format(\"delta\").load(newsapi_path)\n",
    "newsdata_df    = spark.read.format(\"delta\").load(newsdata_path)\n",
    "newsdatahub_df = spark.read.format(\"delta\").load(newsdatahub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "435bcd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_utc_timestamp, lit, array\n",
    "\n",
    "newsapi_silver = (\n",
    "    newsapi_df\n",
    "    .withColumn(\"article_id\", col(\"url\"))\n",
    "    .withColumn(\n",
    "        \"published_at\",\n",
    "        to_utc_timestamp(\n",
    "            to_timestamp(\"publishedAt\"),\n",
    "            \"UTC\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"language\", lit(None).cast(\"string\"))\n",
    "    .withColumn(\"keywords\", lit(None).cast(\"array<string>\"))\n",
    "    .withColumn(\"categories\", lit(None).cast(\"array<string>\"))\n",
    "    .withColumn(\"creator\", array(col(\"author\")))\n",
    "    .withColumn(\"ingestion_source\", lit(\"newsapi\"))\n",
    "    .withColumnRenamed(\"hash\", \"bronze_hash\")\n",
    "    .select(\n",
    "        \"article_id\",\"title\",\"description\",\"content\",\n",
    "        \"published_at\",\"language\",\"url\",\n",
    "        \"keywords\",\"categories\",\"creator\",\n",
    "        \"source_id\",\"source_name\",\n",
    "        \"ingestion_source\",\"bronze_hash\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "561b0d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata_silver = (\n",
    "    newsdata_df\n",
    "    .withColumn(\"article_id\", col(\"article_id\"))\n",
    "    .withColumn(\n",
    "        \"published_at\",\n",
    "        to_utc_timestamp(\n",
    "            to_timestamp(\"pubDate\"),\n",
    "            \"UTC\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"url\", col(\"link\"))\n",
    "    .withColumn(\"categories\", col(\"category\"))\n",
    "    .withColumn(\"ingestion_source\", lit(\"newsdata\"))\n",
    "    .withColumnRenamed(\"hash\", \"bronze_hash\")\n",
    "    .select(\n",
    "        \"article_id\",\"title\",\"description\",\"content\",\n",
    "        \"published_at\",\"language\",\"url\",\n",
    "        \"keywords\",\"categories\",\"creator\",\n",
    "        \"source_id\",\"source_name\",\n",
    "        \"ingestion_source\",\"bronze_hash\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f429af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdatahub_silver = (\n",
    "    newsdatahub_df\n",
    "    .withColumn(\"article_id\", col(\"id\"))\n",
    "    .withColumn(\n",
    "        \"published_at\",\n",
    "        to_utc_timestamp(\n",
    "            to_timestamp(\"pub_date\"),\n",
    "            \"UTC\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"url\", col(\"article_link\"))\n",
    "    .withColumn(\"categories\", col(\"topics\"))\n",
    "    .withColumn(\"creator\", array(col(\"creator\")))\n",
    "    .withColumn(\"keywords\", col(\"keywords\"))\n",
    "    .withColumn(\"country\", array(col(\"source_country\")))\n",
    "    .withColumn(\"ingestion_source\", lit(\"newsdatahub\"))\n",
    "    .withColumnRenamed(\"hash\", \"bronze_hash\")\n",
    "    .select(\n",
    "        \"article_id\",\"title\",\"description\",\"content\",\n",
    "        \"published_at\",\"language\",\"url\",\n",
    "        \"keywords\",\"categories\",\"creator\",\n",
    "        \"source_id\",\"source_title\",\n",
    "        \"ingestion_source\",\"bronze_hash\"\n",
    "    )\n",
    "    .withColumnRenamed(\"source_title\",\"source_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39c8ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_union = (\n",
    "    newsapi_silver\n",
    "    .unionByName(newsdata_silver, allowMissingColumns=True)\n",
    "    .unionByName(newsdatahub_silver, allowMissingColumns=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51d4e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_dedup = (\n",
    "    silver_union\n",
    "    .dropDuplicates([\"url\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1d1475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "silver_path = \"../../sanewsstorage/silver/unified_news/deltatables/articles_silver\"\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "\n",
    "    silver_delta = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "    (\n",
    "        silver_delta.alias(\"t\")\n",
    "        .merge(\n",
    "            silver_dedup.alias(\"s\"),\n",
    "            \"t.bronze_hash = s.bronze_hash\"\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    (\n",
    "        silver_dedup.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(silver_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f33defd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_df = spark.read.format(\"delta\").load(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "155e1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------------------------+----------------------------+--------------------+-------------------+--------+--------------------+--------+----------+---------------+---------+--------------+----------------+--------------------+\n",
      "|          article_id|                                title|                 description|             content|       published_at|language|                 url|keywords|categories|        creator|source_id|   source_name|ingestion_source|         bronze_hash|\n",
      "+--------------------+-------------------------------------+----------------------------+--------------------+-------------------+--------+--------------------+--------+----------+---------------+---------+--------------+----------------+--------------------+\n",
      "|http://9to5google...|                 Samsung is confid...|        Insider reports i...|Insider reports i...|2026-02-03 20:44:18|    NULL|http://9to5google...|    NULL|      NULL|[Andrew Romero]|     NULL|9to5google.com|         newsapi|-2211782981993707329|\n",
      "|http://blog.lived...|   【正論】大石あきこ「NHKよ、党首...|    1:少考さん ★  2026/02...|1: 2026/02/03() 1...|2026-02-03 19:10:17|    NULL|http://blog.lived...|    NULL|      NULL|       [itsoku]|     NULL|   Livedoor.jp|         newsapi|-4519689546209494525|\n",
      "|http://blog.lived...|【正論】東出昌大「クマからしたら人...|    1:少考さん ★  2026/02...|1: 2026/02/04() 2...|2026-02-05 09:40:02|    NULL|http://blog.lived...|    NULL|      NULL|       [itsoku]|     NULL|   Livedoor.jp|         newsapi| 5151717742550277376|\n",
      "|http://blog.lived...|【悲報】英紙「日本人は政策でなくイ...|1:ソリブジン(秋田県) [US]...|1:() [US] 2026/02...|2026-02-08 05:40:06|    NULL|http://blog.lived...|    NULL|      NULL|       [itsoku]|     NULL|   Livedoor.jp|         newsapi| 8810702100873403668|\n",
      "|http://blog.lived...|        “初代Switch”の世界累計販売...|     転載元: https://haya...|: https://hayabus...|2026-02-03 15:08:36|    NULL|http://blog.lived...|    NULL|      NULL|      [nanjstu]|     NULL|   Livedoor.jp|         newsapi| 5214410099647352466|\n",
      "+--------------------+-------------------------------------+----------------------------+--------------------+-------------------+--------+--------------------+--------+----------+---------------+---------+--------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fffc38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "df_fixed = silver_df.withColumn(\"keywords\", concat_ws(\",\", col(\"keywords\")))\n",
    "df_fixed = df_fixed.withColumn(\"categories\", concat_ws(\",\", col(\"categories\")))\n",
    "df_fixed = df_fixed.withColumn(\"creator\", concat_ws(\",\", col(\"creator\")))\n",
    "\n",
    "df_fixed.limit(20).coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"top20_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88059903",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
